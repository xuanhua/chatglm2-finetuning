{
  // Use IntelliSense to learn about possible attributes.
  // Hover to view descriptions of existing attributes.
  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Python Debugger: Current File",
      "type": "debugpy",
      "request": "launch",
      "program": "${file}",
      "console": "integratedTerminal",
      "env": {
        "PYTHONPATH": "/data/xuanhua/hg_models:${workspaceFolder}"
      },
      "justMyCode": false,
    },
    {
      "name": "predict_pretrained.py",
      "type": "debugpy",
      "request": "launch",
      "program": "${workspaceFolder}/inference/predict_pretrained.py",
      "console": "integratedTerminal",
      "env": {
        "PYTHONPATH": ".:/data/xuanhua/hg_models:${workspaceFolder}"
      },
      "justMyCode": false,
      "args": [
        "--test_path",
        "${workspaceFolder}/data/tb_1.jsonl",
        "--device",
        "1",
        "--result_path",
        "${workspaceFolder}/data/output/tb_1_result_chatglm2.txt"
      ]
    },
    {
      "name": "predict_pretrained.py with finetuned model",
      "type": "debugpy",
      "request": "launch",
      "program": "${workspaceFolder}/inference/predict_pretrained.py",
      "console": "integratedTerminal",
      "env": {
        "PYTHONPATH": ".:/data/xuanhua/hg_models:${workspaceFolder}"
      },
      "justMyCode": false,
      "args": [
        "--model_dir",
        "/data/xuanhua/chatglm2-finetuned-models/output_freeze/global_step-449",
        "--test_path",
        "${workspaceFolder}/data/tb_1.jsonl",
        "--device",
        "1",
        "--result_path",
        "${workspaceFolder}/data/output/tb_1_result_chatglm2_with_finetuned_model.txt"
      ]
    },
    {
      "name": "finetuning_freeze.py",
      "type": "debugpy",
      "request": "launch",
      "program": "/home/ubuntu/anaconda3/bin/deepspeed",
      "cwd": "${workspaceFolder}",
      "args": [
        "--num_gpus",
        "1",
        "--master_port",
        "6666",
        "train/finetuning_freeze.py"
      ],
      "console": "integratedTerminal",
      "justMyCode": false
    },
    {
      "name": "deepseek coder pipeline based lora",
      "type": "debugpy",
      "request": "launch",
      "program": "/home/ubuntu/anaconda3/bin/deepspeed",
      "cwd": "${workspaceFolder}",
      "args": [
        "--master_port",
        "5524",
        "${workspaceFolder}/train/finetuning_deepseek_coder_with_pipeline.py",
        "--train_path",
        "${workspaceFolder}/data/dsc_train.jsonl",
        "--per_device_train_batch_size",
        "1",
        "--max_len",
        "768",
        "--max_src_len",
        "450",
        "--num_train_epochs",
        "5",
        "--gradient_accumulation_steps",
        "1",
        "--seed",
        "1234",
        "--show_loss_step",
        "20",
        "--num_stages",
        "2",
        "--save_model_step",
        "300",
        "--output_dir",
        "/data/xuanhua/deepseek_coder_finetuned_models/output_dir_pipeline"
      ],
      "env": {
        "CUDA_VISIBLE_DEVICES":"0,1",
        "TOKENIZERS_PARALLELISM":"false"
      },
      "console": "integratedTerminal",
      "justMyCode": false
    },
    {
      "name": "pipeline based lora training",
      "type": "debugpy",
      "request": "launch",
      "program": "/home/ubuntu/anaconda3/bin/deepspeed",
      "cwd": "${workspaceFolder}",
      "args": [
        "--master_port",
        "5524",
        "${workspaceFolder}/train/finetuning_lora_with_pipeline.py",
        "--train_path",
        "${workspaceFolder}/data/tb_0.jsonl",
        "--per_device_train_batch_size",
        "1",
        "--max_len",
        "768",
        "--max_src_len",
        "450",
        "--num_train_epochs",
        "5",
        "--gradient_accumulation_steps",
        "1",
        "--seed",
        "1234",
        "--show_loss_step",
        "20",
        "--num_stages",
        "2",
        "--save_model_step",
        "300",
        "--output_dir",
        "/data/xuanhua/chatglm2-finetuned-models/output_dir_pipeline"
      ],
      "env": {
        "CUDA_VISIBLE_DEVICES":"0,1"
      },
      "console": "integratedTerminal",
      "justMyCode": false
    },
    {
      "name": "restore_finetuned_model_to_hg_style.py",
      "type": "debugpy",
      "request": "launch",
      "program": "${workspaceFolder}/restore_finetuned_model_to_hg_style.py",
      "cwd": "${workspaceFolder}",
      "args": [
        "--finetuned-model-path",
        "/data/xuanhua/chatglm2-finetuned-models/output_freeze/global_step-449"
      ],
      "console": "integratedTerminal",
      "justMyCode": false
    },
    {
      "name": "merge_lora_adapter_to_hg_model.py",
      "type": "debugpy",
      "request": "launch",
      "program": "${workspaceFolder}/merge_lora_adapter_to_hg_model.py",
      "cwd": "${workspaceFolder}",
      "args": [
        "--checkpoint_path",
        "/data/xuanhua/chatglm2-finetuned-models/output_dir_pipeline/global_step4500",
        "--merged_model_path",
        "/data/xuanhua/chatglm2-finetuned-models/output_dir_pipeline/lora_merged_global_step4500"
      ],
      "console": "integratedTerminal",
      "justMyCode": false
    }

    {
      "name": "deepspeed dump lora adapter from pipelinemodule",
      "type": "debugpy",
      "request": "launch",
      "program": "/home/ubuntu/anaconda3/bin/deepspeed", // The pipeline based model training relies on deepspeed's lauching mode, it doesnot work without distributed backend.
      "cwd": "${workspaceFolder}",
      "args": [
        "--master_port",
        "5524",
        "${workspaceFolder}/save_lora_adapter_from_pipelinemodule.py",
        "--checkpoint_path",
        "/data/xuanhua/chatglm2-finetuned-models/output_dir_pipeline/global_step4500",
        "--num_stages",
        "1"
      ],
      "env": {
        "CUDA_VISIBLE_DEVICES":"0"
      },
      "console": "integratedTerminal",
      "justMyCode": false
    },
    {
      "name": "utils.py",
      "type": "debugpy",
      "request": "launch",
      "python": "/home/ubuntu/anaconda3/bin/python",
      "module": "chatglm2_finetuning.utils",
      "cwd": "${workspaceFolder}/../",
      "console": "integratedTerminal",
      "justMyCode": false
    }
  ]
}